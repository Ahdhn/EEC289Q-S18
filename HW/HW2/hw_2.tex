\documentclass[12pt] {article}
\usepackage{times}
\usepackage[margin=1in,bottom=1in,top=0.6in]{geometry}

\usepackage{hhline}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[inline,shortlabels]{enumitem}%enumerate with letters
\usepackage{mathrsfs} 
\usepackage[square,numbers]{natbib}
\usepackage{graphicx}
\bibliographystyle{unsrtnat}

\usepackage[framed,numbered,autolinebreaks,useliterate]{../mcode}

\begin{document}

\title{EEC 289Q â€“ Data Analytics for Computer Engineers \\ Homework 2}
\author{Ahmed Mahmoud}
\date{April, 29th 2018} 

\maketitle




%============Table========
%\begin{figure}[tbh]
% \centering    
%\begin{tabular}{ |p{4cm}|| p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
% \hline
% & Processor 1 &  Processor 2  & Processor 3 & Processor 4\\ \hhline{|=|=|=|=|=|}
% \hline
% Performance          &$1.08$        &$1.425$       &\textbf{1.52}  &   \\
% \hline
%\end{tabular} 
%\caption{Metric table for the four processors}
%   \label{tab:metric}
%\end{figure} 
%============Figure========
%\begin{figure}[!tbh]
%\centering        
%   \subfloat {\includegraphics[width=0.65\textwidth]{fig2_4.png}}
%   \caption{ }
%   \label{fig:fig}
%\end{figure}


\paragraph{Logistic Regression:} 
The following code show the implementation of the logistic regression function
\begin{lstlisting}
function [f,g] = logistic_regression(theta, X,y)
	m=size(X,2);
	n = size(X,1);  
	f = 0;
	g = zeros(size(theta));

%%% YOUR CODE HERE %%%
	h = sigmoid(theta'*X);
	for i=1:m
   		 f = f - (y(i)*log(h(i)) + (1- y(i))*log(1-h(i)));
	end
	for i=1:m
    	g = g + X(:,i)*(h(i)- y(i));
	end
end	
\end{lstlisting}

Using this code, we were able to achieve training accuracy of $100\%$and 
test accuracy of $100\%$ while the optimization took $5.258526 $ seconds.

The following shows the vectorized version of the same implementation which decreased the optimization time to $2.519300$ seconds. 

\begin{lstlisting}
function [f,g] = logistic_regression_vec(theta, X,y)
	m=size(X,2);.
	f = 0;
	g = zeros(size(theta));

	%%% YOUR CODE HERE %%%
	h = sigmoid(theta'*X);
	f = -sum(y.*log(h) + (1.- y).*(log(1.-h)));
	g= X*(h - y)';
end 
\end{lstlisting}

\paragraph{Linear Regression:}
The following code show the initial implementation of the linear regression method with which the optimization took $0.017464$ seconds

\begin{lstlisting}
function [f,g] = linear_regression(theta, X,y)
	m=size(X,2);
	n=size(X,1);
	f=0;
	g=zeros(size(theta));
%%% YOUR CODE HERE %%%
    err=theta'*X-y;
    for i=1:m
        f = f + 0.5*err(i)*err(i);
    end
    for i=1:n
        g(i) = sum(X(i,:).*err);
    end   
end
\end{lstlisting}

The following is the vectorized version of the linear regression. With this code the optimization took $0.014477$ seconds. The different is small between the vectorized and initial implementation since the number of parameters is small i.e., 14. 

\begin{lstlisting}
function [f,g] = linear_regression_vec(theta, X,y)
	m=size(X,2);  
	f = 0;
	g = zeros(size(theta));  
	%%% YOUR CODE HERE %%%
	err=theta'*X-y;
	f=1/2*err*err';
	g=X*err';
end
\end{lstlisting}

\paragraph{Gradient Testing}

\begin{figure}[!tbh]
 \centering    
\begin{tabular}{ |p{1.5cm}|| p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
 \hline
 &  \texttt{linear regression} & \texttt{linear regression vec}  & \texttt{logistic regression} & \texttt{logistic regression vec}\\
  \hhline{|=|=|=|=|=|}
 \hline


 Test$\#$1  &3.97861e &1.39289e-10&  &   \\
 Test$\#$1  &4.40389e &4.15336e-10&  &   \\
 Test$\#$3  &1.76783e &8.01214e-10&  &   \\
 Test$\#$4  &3.97861e &4.88868e-10&  &   \\
 Test$\#$5  &2.07973e &8.01214e-10&  &   \\
 Test$\#$6  &3.97861e &4.15336e-10&  &   \\
 Test$\#$7  &1.15902e &4.15336e-10&  &   \\
 Test$\#$8  &2.07973e &2.30699e-10&  &   \\
 Test$\#$9  &1.50102e &7.99005e-11&  &   \\
 Test$\#$10 &2.83986e &7.99005e-11&  &   \\

\hline
\end{tabular} 
\caption{Metric table for the four processors}
   \label{tab:gtest}
\end{figure} 


\end{document}