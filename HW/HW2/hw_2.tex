\documentclass[12pt] {article}
\usepackage{times}
\usepackage[margin=1in,bottom=1in,top=0.6in]{geometry}

\usepackage{hhline}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[inline,shortlabels]{enumitem}%enumerate with letters
\usepackage{mathrsfs} 
\usepackage[square,numbers]{natbib}
\usepackage{graphicx}
\bibliographystyle{unsrtnat}
\usepackage{float}
\usepackage[framed,numbered,autolinebreaks,useliterate]{../mcode}

\begin{document}

\title{EEC 289Q â€“ Data Analytics for Computer Engineers \\ Homework 2}
\author{Ahmed Mahmoud}
\date{April, 29th 2018} 

\maketitle




%============Table========
%\begin{table}[tbh]
% \centering    
%\begin{tabular}{ |p{4cm}|| p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
% \hline
% & Processor 1 &  Processor 2  & Processor 3 & Processor 4\\ \hhline{|=|=|=|=|=|}
% \hline
% Performance          &$1.08$        &$1.425$       &\textbf{1.52}  &   \\
% \hline
%\end{tabular} 
%\caption{Metric table for the four processors}
%   \label{tab:metric}
%\end{table} 
%============Figure========
%\begin{figure}[!tbh]
%\centering        
%   \subfloat {\includegraphics[width=0.65\textwidth]{fig2_4.png}}
%   \caption{ }
%   \label{fig:fig}
%\end{figure}


\paragraph{Logistic Regression:} 
The following code shows the implementation of the logistic regression function
\begin{lstlisting}
function [f,g] = logistic_regression(theta, X,y)
	m = size(X,2);
	n = size(X,1);  
	f = 0;
	g = zeros(size(theta));
%%% YOUR CODE HERE %%%
	h = sigmoid(theta'*X);
	for i=1:m
   		 f = f - (y(i)*log(h(i)) + (1- y(i))*log(1-h(i)));
	end
	for i=1:m
    	g = g + X(:,i)*(h(i)- y(i));
	end
end	
\end{lstlisting}

Using this code, we were able to achieve training accuracy of $100\%$ and 
test accuracy of $100\%$ while the optimization took $5.258526 $ seconds.

The following shows the vectorized version of the same implementation which decreased the optimization time to $2.519300$ seconds. 

\begin{lstlisting}
function [f,g] = logistic_regression_vec(theta, X,y)
	m = size(X,2);.
	f = 0;
	g = zeros(size(theta));
	%%% YOUR CODE HERE %%%
	
	h = sigmoid(theta'*X);
	f = -sum(y.*log(h) + (1.- y).*(log(1.-h)));
	g = X*(h - y)';
end 
\end{lstlisting}
\newpage
\paragraph{Linear Regression:}
The following code shows the initial implementation of the linear regression method (Homework \#1) with which the optimization took $0.017464$ seconds

\begin{lstlisting}
function [f,g] = linear_regression(theta, X,y)
	m = size(X,2);
	n = size(X,1);
	f = 0;
	g = zeros(size(theta));
%%% YOUR CODE HERE %%%
    err = theta'*X-y;
    for i = 1:m
        f = f + 0.5*err(i)*err(i);
    end
    for i = 1:n
        g(i) = sum(X(i,:).*err);
    end   
end
\end{lstlisting}

The following is the vectorized version of the linear regression. With this code, the optimization took $0.014477$ seconds. The different is small between the vectorized and initial implementation since the number of parameters is small i.e., 14. 

\begin{lstlisting}
function [f,g] = linear_regression_vec(theta, X,y)
	m=size(X,2);  
	f = 0;
	g = zeros(size(theta));  
	%%% YOUR CODE HERE %%%
	err=theta'*X-y;
	f=1/2*err*err';
	g=X*err';
end
\end{lstlisting}
\newpage

\paragraph{Gradient Checking:}
We tested the gradient computation for the four methods and compared it against the approximate gradient to calculate the absolute error. Table ~\ref{tab:gtest} shows the absolute error for the four methods where our computed gradient matches the approximated gradient with very small error of order $10^{-10}$.

\begin{table}[H]
 \centering    
\begin{tabular}{ |p{1.4cm}|| p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
 \hline
 &  \texttt{linear regression} & \texttt{linear regression vec}  & \texttt{logistic regression} & \texttt{logistic regression vec}\\
  \hhline{|=|=|=|=|=|}
 \hline 
 Test$\#$1  & 5.47473e-12 & 5.08606e-11  & 3.35132e-13 & 4.90073e-15   \\
 Test$\#$1  & 5.47473e-12 &           0  & 3.25036e-14 &  4.2404e-12   \\
 Test$\#$3  & 3.18963e-11 & 2.91038e-11  & 5.95617e-14 &           0   \\
 Test$\#$4  & 2.28511e-11 & 7.28164e-11  & 6.31256e-14 &           0   \\
 Test$\#$5  & 1.04592e-11 & 4.00178e-11  &           0 & 7.15943e-14   \\
 Test$\#$6  & 6.87805e-12 & 2.91038e-11  & 7.72179e-15 & 8.13418e-13   \\
 Test$\#$7  &           0 & 7.28164e-11  &           0 & 4.60755e-14   \\
 Test$\#$8  & 1.54614e-11 &  8.0469e-13  & 2.75524e-13 & 4.49618e-14   \\
 Test$\#$9  & 5.47473e-12 & 7.28164e-11  & 4.00648e-14 & 2.12491e-13   \\
 Test$\#$10 &12.54659e-11 &           0  & 2.04076e-12 & 1.21385e-14   \\
\hline
\end{tabular} 
\caption{Gradient Checking: The absolute error between computed gradient for different methods vs. the approximated gradient.}
   \label{tab:gtest}
\end{table}


\end{document}