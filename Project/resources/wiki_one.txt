The task of designing deep neural network sutiabel for 3D data is non trivial. For example, most information in RGB images is encoded as pixels and pixels intensity distribution for each color channel. However, information of 3D models reside on the surface, unlike 2D pixels images. Therefore, feautre useful for 2D image classification might not necessarily be sufficient for 3D models classification. CAD models are often colorless and therefore the models must be able to pick up on other features. 

Standard deep neural network models take as input data with regular structures while point clouds are fundamentally irregular.: point positions are continuously distributed in the space and any permutation of their ordering does not change the spatial distribution. One common approach to process point cloud data using deep learning models is to first convert raw point cloud data into volumetric representation, namely 3D grid. This approach, however, usually introduces quantization artifacts and excessive memory usage making it difficult to go to cpature high-resolution or fine-grained. 