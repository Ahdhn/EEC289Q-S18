\section{Results}
%exp2
We have conducted different experiments starting with the model shown in Figure~\ref{fig:model1} where we have applied several modification to it to obtain higher performance in terms of accuracy and/or training time. We started without including the point normal information. We observed a large gap between train accuracy and test accuracy indicating an over-fitting. After 200 epochs, the train accuracy was $98\%$ while the test accuracy was $91\%$. Additionally, the model was too slow as it could take upto a day to complete the first 150 epochs on Nvidia Titan V GPU. 

%exp3
The fully connected layers were the bottleneck of our models. The huge number of weights introduced during these two fully connected layers takes up all the computation time. For that, we removed one of them (FC1). Additionally, we decreased the number of channels in each of the first four convolutional layers to be (16, 32, 64, 128). Finally, we reduced the dropout rate to be 0.2 instead of 0.5. This reduced the training time significantly as it could take 8 hours to train the first 150 epochs. The test accuracy was not affected by these changes; it became $90.07\%$. Surprisingly, this also helped the decreasing the gap between the train accuracy and test accuracy where the train accuracy was $87\%$. 

%exp4
Going to back to how we extracted the features, we could notice that each of the feature layers (labeled as \textbf{EdgeFeatures\_XX} in Figure~\ref{fig:model1} has duplicated information. Each point concatenates its coordinates along with the difference in coordinates with one of its $K$ neighbors. This means that the single point coordinates is being feed to the CNN model $K$ times for each layers. For that, we concatenated the point coordinates only once when $K$ is largest. For other layers, only the local information was feed in. This helped reducing the train time even further. It only took 4 hours to train first 150 epochs. The test accuracy slightly increased to be $90.3\%$ and the train accuracy increased to be $94.5\%$ 

%epx5/exp6/exp7
We now introduce the points normal features and feed it to the CNN model. We do this by replacing \textbf{EdgeFeatures\_10} and \textbf{EdgeFeatures\_30} to be \textbf{NormFeatures\_20} and \textbf{NormFeatures\_40}. That means we take the point and normal information from two set of neighborhood for $K=20$ and $K=40$. We allow concatenation of the point feature and normal feature only for layers with $K=40$. The training time is reduced even further to be 2.5 hours for the first 150 epochs. This is due to the fact the NormFeatures tuple per point is 4 while the EdgeFeatures is 6 which reduced the number of weights. In addition, the train accuracy increased to be $90.6\%$. With further experimentation, we found that we can increase the test accuracy to be $91.5\%$ if we increased the number of channels in the first three convolutional layers to be 64 while increasing the number of channels in the aggregation and fully connected layer to be 512. However, this model take longer to train; more than 5 hours for the first 100 epochs. 